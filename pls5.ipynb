{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, validation=False):\n",
    "        self.images = []\n",
    "        self.boxes = []\n",
    "\n",
    "\n",
    "        x_final, y_final = 600, 1200\n",
    "        transform = Resize((y_final, x_final))\n",
    "        to_tensor = ToTensor()\n",
    "        for item in data:\n",
    "            file_name = item['data']['image'].split('-')[-1]\n",
    "\n",
    "            image_path = file_name  # Zmień odpowiednio ścieżkę do katalogu ze zdjęciami\n",
    "            image = Image.open(image_path)\n",
    "            image = to_tensor(transform(image))*255\n",
    "            image = image.type(torch.uint8)\n",
    "            self.images.append(image)\n",
    "            points = item['annotations'][0]['result'][0]['value']['points']\n",
    "\n",
    "            xmin = min(point[0] for point in points)/100*x_final\n",
    "            ymin = min(point[1] for point in points)/100*y_final\n",
    "            xmax = max(point[0] for point in points)/100*x_final\n",
    "            ymax = max(point[1] for point in points)/100*y_final\n",
    "            box = torch.tensor([[xmin, ymin, xmax, ymax]])\n",
    "            self.boxes.append(box)\n",
    "\n",
    "        self.labels = torch.ones((len(data), 1), dtype=torch.int64)*84\n",
    "\n",
    "\n",
    "        transforms = [T.ElasticTransform(alpha=45.0), #T.Grayscale(),\n",
    "                      T.ColorJitter(brightness=.5, hue=.3),\n",
    "                      T.RandomInvert(), T.RandomPosterize(bits=2),\n",
    "                      T.RandomSolarize(threshold=0.5),\n",
    "                      T.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                      T.RandomAutocontrast(),\n",
    "                      T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                      ]\n",
    "        self.transforms = T.Compose([*[T.RandomApply([transform], p=0.5) for transform in transforms]])\n",
    "        if validation:\n",
    "            self.transforms = None\n",
    "        self.transforms = None\n",
    "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        self.normalize = lambda x: x\n",
    "\n",
    "    # def crop_img_with_box(self, img, box):\n",
    "    #     # TODO: crop image with box\n",
    "    #     return img, box\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_img = self.images[idx]\n",
    "        if self.transforms is not None:\n",
    "            out_img = self.transforms(out_img)\n",
    "\n",
    "        out_box = self.boxes[idx]\n",
    "        # out_img, out_box = self.crop_img_with_box(out_img, out_box)\n",
    "        return self.normalize(out_img/255), out_box, self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('labelj.json') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT, box_score_thresh=0.5)\n",
    "weights_dict = dict(model.named_parameters())\n",
    "for k, v in weights_dict.items():\n",
    "    if \"box_predictor\" not in k:\n",
    "        v.requires_grad = False\n",
    "    else:\n",
    "        v.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1e582b60e84ad78d67c1b77565127b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3632, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2971, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3613, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4257, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3547, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.5531, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4823, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3055, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3077, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.5617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.1988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3056, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4988, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2336, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.6625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4114, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2109, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3243, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3108, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2996, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2917, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2733, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4174, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2958, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2394, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3071, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2878, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4259, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3600, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3015, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2730, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2811, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2769, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2999, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2147, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3862, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4162, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3367, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3455, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3860, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2322, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4423, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3471, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3352, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.6344, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Hiperparametr\n",
    "epochs = 20\n",
    "#lr, momentum - hiperparametry\n",
    "#można spróbować torch.optim.Adam\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.000, momentum=0.0)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "training_dataset = TrainingDataset(data)\n",
    "#batch_size = 2 - hiperparametr\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=2, shuffle=True)\n",
    "val_dataset = TrainingDataset(data, validation=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for epoch in tqdm(list(range(epochs))):\n",
    "    model.train()\n",
    "    for images, boxes, labels in training_dataloader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = []\n",
    "        for i in range(len(images)):\n",
    "            d = {}\n",
    "            d['boxes'] = boxes[i].to(device)\n",
    "            d['labels'] = labels[i].to(device)\n",
    "            targets.append(d)\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(losses)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, boxes, labels in val_dataloader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "\n",
    "            preds = model(images)\n",
    "\n",
    "#Zapisanie modelu\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tutaj są predykcje dla wytrenowanego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 29\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m# if boxes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#     boxes = torch.stack(boxes)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m#     box_im = draw_bounding_boxes(box_im, boxes=boxes,\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[39m#                           colors=\"green\",\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m#                           width=4)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m im \u001b[39m=\u001b[39m to_pil_image(box_im\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m---> 29\u001b[0m im\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2486\u001b[0m, in \u001b[0;36mImage.show\u001b[1;34m(self, title)\u001b[0m\n\u001b[0;32m   2466\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(\u001b[39mself\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2467\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[39m    Displays this image. This method is mainly intended for debugging purposes.\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2483\u001b[0m \u001b[39m    :param title: Optional title to use for the image window, where possible.\u001b[39;00m\n\u001b[0;32m   2484\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2486\u001b[0m     _show(\u001b[39mself\u001b[39;49m, title\u001b[39m=\u001b[39;49mtitle)\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3524\u001b[0m, in \u001b[0;36m_show\u001b[1;34m(image, **options)\u001b[0m\n\u001b[0;32m   3521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_show\u001b[39m(image, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions):\n\u001b[0;32m   3522\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageShow\n\u001b[1;32m-> 3524\u001b[0m     ImageShow\u001b[39m.\u001b[39;49mshow(image, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:62\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(image, title, **options)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39mDisplay a given image.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m:returns: ``True`` if a suitable viewer was found, ``False`` otherwise.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m viewer \u001b[39min\u001b[39;00m _viewers:\n\u001b[1;32m---> 62\u001b[0m     \u001b[39mif\u001b[39;00m viewer\u001b[39m.\u001b[39;49mshow(image, title\u001b[39m=\u001b[39;49mtitle, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions):\n\u001b[0;32m     63\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:86\u001b[0m, in \u001b[0;36mViewer.show\u001b[1;34m(self, image, **options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39mmode \u001b[39m!=\u001b[39m base:\n\u001b[0;32m     84\u001b[0m         image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mconvert(base)\n\u001b[1;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshow_image(image, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:112\u001b[0m, in \u001b[0;36mViewer.show_image\u001b[1;34m(self, image, **options)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_image\u001b[39m(\u001b[39mself\u001b[39m, image, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions):\n\u001b[0;32m    111\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display the given image.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshow_file(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_image(image), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:129\u001b[0m, in \u001b[0;36mViewer.show_file\u001b[1;34m(self, path, **options)\u001b[0m\n\u001b[0;32m    127\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 129\u001b[0m os\u001b[39m.\u001b[39msystem(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_command(path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions))  \u001b[39m# nosec\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "import torch\n",
    "\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "for img, boxes, labels in val_dataset:\n",
    "    preds = model([img])\n",
    "    img = img*255\n",
    "    img = img.type(torch.uint8)\n",
    "    box_im = draw_bounding_boxes(img, boxes=boxes,\n",
    "\n",
    "                              colors=\"red\",\n",
    "                              width=4)\n",
    "    # boxes = [box for box, label, score in el.items() if label == 84 for el in preds]\n",
    "    boxes = []\n",
    "    for el in preds:\n",
    "        for box, label, score in zip(el['boxes'], el['labels'], el['scores']):\n",
    "            if label == 84:\n",
    "                boxes.append(box)\n",
    "    # if boxes:\n",
    "    #     boxes = torch.stack(boxes)\n",
    "    #     box_im = draw_bounding_boxes(box_im, boxes=boxes,\n",
    "\n",
    "    #                           colors=\"green\",\n",
    "    #                           width=4)\n",
    "    im = to_pil_image(box_im.detach())\n",
    "    im.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tutaj są predykcje dla modelu bez zmian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m     box_im \u001b[39m=\u001b[39m draw_bounding_boxes(box_im, boxes\u001b[39m=\u001b[39mboxes,\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m                           colors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m                           width\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m     26\u001b[0m im \u001b[39m=\u001b[39m to_pil_image(box_im\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m---> 27\u001b[0m im\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2486\u001b[0m, in \u001b[0;36mImage.show\u001b[1;34m(self, title)\u001b[0m\n\u001b[0;32m   2466\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(\u001b[39mself\u001b[39m, title\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2467\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[39m    Displays this image. This method is mainly intended for debugging purposes.\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2483\u001b[0m \u001b[39m    :param title: Optional title to use for the image window, where possible.\u001b[39;00m\n\u001b[0;32m   2484\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2486\u001b[0m     _show(\u001b[39mself\u001b[39;49m, title\u001b[39m=\u001b[39;49mtitle)\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:3524\u001b[0m, in \u001b[0;36m_show\u001b[1;34m(image, **options)\u001b[0m\n\u001b[0;32m   3521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_show\u001b[39m(image, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions):\n\u001b[0;32m   3522\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageShow\n\u001b[1;32m-> 3524\u001b[0m     ImageShow\u001b[39m.\u001b[39;49mshow(image, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:62\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(image, title, **options)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39mDisplay a given image.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m:returns: ``True`` if a suitable viewer was found, ``False`` otherwise.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m viewer \u001b[39min\u001b[39;00m _viewers:\n\u001b[1;32m---> 62\u001b[0m     \u001b[39mif\u001b[39;00m viewer\u001b[39m.\u001b[39;49mshow(image, title\u001b[39m=\u001b[39;49mtitle, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions):\n\u001b[0;32m     63\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:86\u001b[0m, in \u001b[0;36mViewer.show\u001b[1;34m(self, image, **options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39mmode \u001b[39m!=\u001b[39m base:\n\u001b[0;32m     84\u001b[0m         image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mconvert(base)\n\u001b[1;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshow_image(image, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:112\u001b[0m, in \u001b[0;36mViewer.show_image\u001b[1;34m(self, image, **options)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_image\u001b[39m(\u001b[39mself\u001b[39m, image, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions):\n\u001b[0;32m    111\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display the given image.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshow_file(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_image(image), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\Boows\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageShow.py:129\u001b[0m, in \u001b[0;36mViewer.show_file\u001b[1;34m(self, path, **options)\u001b[0m\n\u001b[0;32m    127\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 129\u001b[0m os\u001b[39m.\u001b[39msystem(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_command(path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions))  \u001b[39m# nosec\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT, box_score_thresh=0.5)\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "for img, boxes, labels in val_dataset:\n",
    "    preds = model([img])\n",
    "    img = img*255\n",
    "    img = img.type(torch.uint8)\n",
    "    box_im = draw_bounding_boxes(img, boxes=boxes,\n",
    "\n",
    "                              colors=\"red\",\n",
    "                              width=4)\n",
    "    # boxes = [box for box, label, score in el.items() if label == 84 for el in preds]\n",
    "    boxes = []\n",
    "    for el in preds:\n",
    "        for box, label, score in zip(el['boxes'], el['labels'], el['scores']):\n",
    "            if label == 84:\n",
    "                boxes.append(box)\n",
    "    if boxes:\n",
    "        boxes = torch.stack(boxes)\n",
    "        box_im = draw_bounding_boxes(box_im, boxes=boxes,\n",
    "\n",
    "                              colors=\"green\",\n",
    "                              width=4)\n",
    "    im = to_pil_image(box_im.detach())\n",
    "    im.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
